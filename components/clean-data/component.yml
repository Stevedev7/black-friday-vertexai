name: Clean data
inputs:
- {name: dataset, type: Dataset}
outputs:
- {name: cleaned_data, type: Artifact}
- {name: column_transformer, type: Artifact}
- {name: label_encoder, type: Artifact}
- {name: dataset_data, type: Markdown}
implementation:
  container:
    image: python:3.8-slim
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'np' 'sklearn' 'fsspec' 'gcsfs' 'kfp==1.8.9' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef clean_data(\n    dataset: Input[Dataset],\n    cleaned_data:\
      \ Output[Artifact],\n    column_transformer: Output[Artifact],\n    label_encoder:\
      \ Output[Artifact],\n    dataset_data: Output[Markdown]\n):\n    import json\n\
      \    import numpy as np\n    import pandas as pd\n    from pickle import dump\n\
      \    from sklearn.impute import SimpleImputer\n    from sklearn.compose import\
      \ ColumnTransformer\n    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\
      \n    df = pd.read_csv(dataset.uri)\n    df.drop(['User_ID', 'Product_ID', 'Product_Category_3'],\
      \ axis=1, inplace=True)\n    # Split the data into independent and dependent\
      \ variables and remove unnecessary columns\n    x = df.iloc[:, :-1].values\n\
      \    y = df.iloc[:, -1].values\n\n    json_data = json.dumps({\"dataset\":[{_:\
      \ df[_].unique().tolist()}for _ in df.drop(['Purchase'], axis=1).columns.tolist()]},\
      \ indent=4)\n\n    markdown_content = f\"\"\"\n    ```json\n    {json_data}\n\
      \n    ```\n    \"\"\"\n\n    with open(dataset_data.path, 'w') as f:\n     \
      \   f.write(markdown_content)\n\n    #Encode Gender column\n    le = LabelEncoder()\n\
      \    x[:, 0] = le.fit_transform(x[:, 0])\n\n    # Fill NaN values in Product_Category_2\
      \ column \n    imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n\
      \    x[:, 7] = imputer.fit_transform(x[:, 7].reshape(-1, 1)).reshape(1, -1)[0]\n\
      \n    # Onehot encode the Age, City_Category, Stay_In_Current_City_Years columns\n\
      \    ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1, 3,\
      \ 4])], remainder='passthrough')\n    x = ct.fit_transform(x)\n\n    # Merge\
      \ the dataset to pass it to the next component\n    x = pd.DataFrame(x)\n  \
      \  y = pd.DataFrame(y)\n    data = pd.concat([x,y], axis=1)\n\n    # Write the\
      \ data to a csv file\n    data.to_csv(path_or_buf=cleaned_data.path, index=False)\n\
      \n    # Write the column transformer object into a file\n    with open(column_transformer.path,\
      \ 'wb') as output_file:\n        dump(ct, output_file)\n\n    with open(label_encoder.path,\
      \ 'wb') as output_file:\n        dump(le, output_file)\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - clean_data
